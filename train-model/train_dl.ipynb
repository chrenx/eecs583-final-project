{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.__version__\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils and Input & Output Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Utils =========================================\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    if ('val_categorical_accuracy' in history.history.keys()):\n",
    "        plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    if ('val_loss' in history.history.keys()):\n",
    "        plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def adBits(x, x2, y, seqsize=100):\n",
    "   for i in range(x.shape[0]):\n",
    "      #print(' for each train sample  ... ', i)\n",
    "      loop = seqsize\n",
    "      j = 0\n",
    "      for loop in range(seqsize):     \n",
    "          #print(' for each node ... ', j)\n",
    "          adBits_1 = int(x[i][2*loop])\n",
    "          #Get the next LONG since each node now has 2 LONGs for adj edges\n",
    "          adBits_2 = int(x[i][2*loop+1])\n",
    "          # Uncomment below if you want invalid nodes to not feed to NN before any valid nodes\n",
    "          #if (adBits_1==0 and adBits_1==0 and loop!=seqsize-1):              \n",
    "          #    y[i][j] = y[i][loop+1] \n",
    "          #    continue\n",
    "          if (adBits_1):\n",
    "              #print adBits\n",
    "              for k in range(0,64):\n",
    "                  bit = adBits_1 & 1\n",
    "                  #print ( ' adBits&1 = ',  bit )\n",
    "                  # Uncomment below if you want new nodes to only have adjacency to earlier nodes\n",
    "                  if ( bit == 1):\n",
    "                  #if ( bit == 1 and k<j):\n",
    "                      x2[i][j][k] = 1\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  else:\n",
    "                      x2[i][j][k] = 0\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  adBits_1 >>= 1\n",
    "              x2[i][j][j] = 1 \n",
    "\n",
    "          if (adBits_2):   \n",
    "              #print adBits\n",
    "              for k in range(64,100):\n",
    "                  bit = adBits_2 & 1\n",
    "                  #print ( ' adBits&1 = ',  bit )\n",
    "                  if ( bit == 1):\n",
    "                  #if ( bit == 1 and k<j):\n",
    "                      x2[i][j][k] = 1\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  else:\n",
    "                      x2[i][j][k] = 0\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  adBits_2 >>= 1\n",
    "              x2[i][j][j] = 1\n",
    "          j = j+1\n",
    "   return x2, y\n",
    "\n",
    "def updateLabelBits(x, y, seqsize=100):\n",
    "    for i in range(x.shape[0]):\n",
    "        #print(' for each train sample  ... ', i)\n",
    "        label_dict = {}\n",
    "        label_num = 1\n",
    "        for j in range(seqsize):\n",
    "            if (x[i][j][j] == 0):\n",
    "                y[i][j] = 0\n",
    "            else:\n",
    "                if y[i][j] in label_dict:\n",
    "                    y[i][j] = label_dict[y[i][j]]\n",
    "                else:\n",
    "                    label_dict[y[i][j]] = label_num\n",
    "                    y[i][j] = label_num\n",
    "                    label_num = label_num + 1\n",
    "    return y\n",
    "\n",
    "# ============================== Post Process ==================================\n",
    "\n",
    "\n",
    "\n",
    "def post_process (x2_pred, predicted, seqsize=100):\n",
    "    #Calculate the number of edges which will require correction\n",
    "    invCols = 0\n",
    "    edges = 0\n",
    "    for i in range(x2_pred.shape[0]):\n",
    "        for j in range(seqsize): # row\n",
    "            for k in range(j): # col\n",
    "                adj = x2_pred[i][j][k]\n",
    "                if (adj == 1):\n",
    "                    edges += 1\n",
    "                    if (np.argmax(predicted[i][j]) == np.argmax(predicted[i][k])):\n",
    "                        invCols += 1\n",
    "                        \n",
    "    print('Total No of edges ', edges)\n",
    "    print('# of edges with invalid coloring ', invCols)\n",
    "    print('Total percentage of edges with invalid colors ', invCols/edges)\n",
    "\n",
    "def post_process_chromatic (x2_pred, predicted, seqsize=100):  \n",
    "    colors_list_list = []\n",
    "    for i in range(x2_pred.shape[0]):\n",
    "        colors_list = []\n",
    "        for j in range(seqsize):\n",
    "            # Valid nodes will have below set to 1 so check the color \n",
    "            # assignment of those nodes only\n",
    "            if (x2_pred[i][j][j] != 0):\n",
    "                colors_list.append(np.argmax(predicted[i][j]))\n",
    "        print('Colors list of graph ', i, ' is  \\n', colors_list)\n",
    "        chromatic_number = len(set(colors_list))\n",
    "        print('Chromatic number of graph ', i, ' is  ', chromatic_number)\n",
    "        colors_list_list.append(colors_list)\n",
    "    return colors_list_list\n",
    "\n",
    "def create_csv_rows(graph_name, colors_list_list_before_correction, \n",
    "                     colors_list_list_after_correction):\n",
    "    csv_rows = []    \n",
    "    for i in range(len(colors_list_list_before_correction)):\n",
    "        row = [graph_name, i, len(set(colors_list_list_before_correction[i])), \n",
    "               len(set(colors_list_list_after_correction[i]))]        \n",
    "        csv_rows.append(row)\n",
    "    return csv_rows\n",
    "\n",
    "def post_process_correction (x2_pred, predicted, colors_list_list, seqsize=100): \n",
    "  totInvCols = 0\n",
    "  totEdges = 0\n",
    "\n",
    "  for i in range(x2_pred.shape[0]):\n",
    "      #maxcol = max(xpredicted[i])\n",
    "      maxcol = max(colors_list_list[i])\n",
    "      #print(maxcol)\n",
    "      #mcol = maxcol[0]\n",
    "      maxorigcol = maxcol\n",
    "      mcolnew = maxcol\n",
    "      #print('Maxcol = ',maxcol[0])\n",
    "      #print(' ... FOR SAMPLE  ... ', i)\n",
    "      invCols = 0\n",
    "      edges = 0;\n",
    "      newCol = 500\n",
    "\n",
    "      for j in range(seqsize):\n",
    "          #print(' ... ... FOR EACH NODE ... ...', j)\n",
    "          for k in range(j):\n",
    "              #print(' ... ... ... for each adjacency  ... ... ...', k)\n",
    "              adj = x2_pred[i][j][k]\n",
    "              #There is an edge\n",
    "              if ( adj == 1 ):\n",
    "                  edges += 1\n",
    "                  if ( np.argmax(predicted[i][j]) == np.argmax(predicted[i][k]) ):                   \n",
    "                      col_j = np.argmax(predicted[i][j])\n",
    "                      col_k = np.argmax(predicted[i][k])\n",
    "                      invCols += 1\n",
    "\n",
    "                      #Check whether we can give one of the existing colors\n",
    "                      foundfinalcol = 0\n",
    "                      for  y in range(1,maxcol+1):\n",
    "                          #print('Check for COLOR NO ... ', y)\n",
    "                          if ( foundfinalcol == 1 ) :\n",
    "                              #print('FOUND COLOR ALREADY  ... leave the loop')\n",
    "                              break\n",
    "\n",
    "                          foundcol = 0\n",
    "                          #Check the adjacent nodes of j\n",
    "                          #for  z in range(j):\n",
    "                          for z in range(seqsize):\n",
    "                              if j!=z:\n",
    "                                  if  (   ((x2_pred[i][j][z] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                      or  ((x2_pred[i][z][j] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                      ):\n",
    "                                      #print('[1] Adjacent node ... from ',j, '-->', z, 'color = ',xpredicted[i][z][0] )\n",
    "                                      foundcol = 1\n",
    "                                      #print('[1] Found Color ', y, ' for node ', z, 'from node ', j )\n",
    "                                      break\n",
    "\n",
    "                          #Finished checking the adjacent nodes of j\n",
    "                          #Color y is not used by any of j's neighbours\n",
    "                          #print('[1] Finished Checking the adjacent node of ... ',j,' ... foundcol = ',foundcol)\n",
    "                          if ( foundcol == 0 ) :\n",
    "                              #assign any prediction > 1\n",
    "                              predicted[i][j][y] = 2\n",
    "                              #print('[1] Reuse color ', y, ' for node ', j)\n",
    "                              foundfinalcol = 1\n",
    "\n",
    "                          else :\n",
    "                              foundcol = 0                                                            \n",
    "                              #Check the adjacent nodes of k\n",
    "                              for z in range(seqsize):\n",
    "                                  if k!=z:\n",
    "                                      if  (   ((x2_pred[i][k][z] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                          or  ((x2_pred[i][z][k] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                          ):\n",
    "                                          #print('[1] Adjacent node ... from ',j, '-->', z, 'color = ',xpredicted[i][z][0] )\n",
    "                                          foundcol = 1\n",
    "                                          #print('[1] Found Color ', y, ' for node ', z, 'from node ', j )\n",
    "                                          break\n",
    "                              #Color y is not used by any of k's neighbours\n",
    "                              if ( foundcol == 0 ) :\n",
    "                                  #assign any prediction > 1\n",
    "                                  predicted[i][k][y] = 2\n",
    "                                  #print('[2] Reuse color ', y, ' for node ', k )\n",
    "                                  foundfinalcol = 1\n",
    "\n",
    "                      # Could not color using an existing color\n",
    "                      # Get a new color from 500 onwards OR use from the new 500 color number series\n",
    "                      if ( foundfinalcol == 0 ) :\n",
    "                           #newCol += 1\n",
    "                           mcolnew += 1\n",
    "                           #assign any prediction > 1\n",
    "                           predicted[i][k][mcolnew] = 2\n",
    "                           maxcol +=1\n",
    "                           #print('Use new color ', mcolnew, ' for node ', k)\n",
    "\n",
    "  return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape 1:  (8158, 200) (8158, 100)\n",
      "shape 2:  (8158, 100, 100)\n",
      "shape 3:  (8158, 100, 100) (8158, 100)\n"
     ]
    }
   ],
   "source": [
    "seq_size = 100\n",
    "seq = pd.read_csv(\"train_data.csv\",header=None,low_memory=False)\n",
    "columns = seq.columns.tolist()\n",
    "\n",
    "#get the adj edges\n",
    "adj_edge = np.array(seq[columns[1:2*seq_size+1]])\n",
    "#get the colors\n",
    "data_color = np.array(seq[columns[2*seq_size+1:3*seq_size+1]])\n",
    "print(\"shape 1: \", adj_edge.shape, data_color.shape)\n",
    "\n",
    "X = np.zeros((adj_edge.shape[0], data_color.shape[1], seq_size))\n",
    "print(\"shape 2: \", X.shape)\n",
    "\n",
    "X, Y = adBits(adj_edge, X, data_color)\n",
    "Y = updateLabelBits(X, Y)\n",
    "\n",
    "\n",
    "print(\"shape 3: \", X.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8158, 100, 100]), torch.Size([8158, 100, 101]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.eye(101, dtype='float32')[Y]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6526, 100, 100]),\n",
       " torch.Size([6526, 100, 101]),\n",
       " torch.Size([1632, 100, 100]),\n",
       " torch.Size([1632, 100, 101]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "x_train, x_test= x_train.to(device), x_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8158, 100, 100)\n",
      "(8158, 100, 101)\n"
     ]
    }
   ],
   "source": [
    "seqsize = 100\n",
    "\n",
    "eseq1 = pd.read_csv(\"train_data.csv\",header=None,low_memory=False)\n",
    "\n",
    "eseq  = eseq1\n",
    "\n",
    "columns = eseq.columns.tolist()\n",
    "#2 entries for each node's adjacency \n",
    "edata = eseq[columns[1:3*seqsize+1]]\n",
    "#get the adj edges\n",
    "edataadj = eseq[columns[1:2*seqsize+1]]\n",
    "#get the colors\n",
    "edatacols = eseq[columns[2*seqsize+1:3*seqsize+1]]\n",
    "#chromatic number\n",
    "ncols = eseq[columns[0:1]]\n",
    "\n",
    "#Now make x_train a 2-dim array\n",
    "#As we have read the adjacency row as bits\n",
    "#and we now want to make them into features\n",
    "#x_train and x2_train have different shapes as 2 LONGs are present in x_train\n",
    "#So use y_train.shape[1] instead of x_train.shape[1]\n",
    "#x2_train = np.zeros((x_train.shape[0],y_train.shape[1],128))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train, y_train = np.array(edataadj), np.array(edatacols)\n",
    "x2_train = np.zeros((x_train.shape[0], y_train.shape[1], seqsize))\n",
    "x2_train, y_train = adBits(x_train, x2_train, y_train)\n",
    "y_train = updateLabelBits(x2_train, y_train)\n",
    "y2_train = to_categorical(y_train, num_classes=101)\n",
    "print(x2_train.shape)\n",
    "print(y2_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 0., 0.]]),\n",
       " array([ 6.,  0., 13.,  0., 11.,  0.]),\n",
       " array([1, 2, 3, 1, 0, 0]),\n",
       " array([[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y2_train[0][0:3]\n",
    "# y_train.shape, y_train[38], y2_train[38][3]\n",
    "x2_train[38, 0:4, 0:6], x_train[38, 0:6], y_train[38, 0:6], y2_train[38, 0:4, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, seq_size=100, device=device):\n",
    "        seq = pd.read_csv(img_dir, header=None, low_memory=False)\n",
    "        columns = seq.columns.tolist()\n",
    "        #get the adj edges\n",
    "        adj_edge = np.array(seq[columns[1:2*seq_size+1]])\n",
    "        #get the colors\n",
    "        data_color = np.array(seq[columns[2*seq_size+1:3*seq_size+1]])\n",
    "        self.X = np.zeros((adj_edge.shape[0], data_color.shape[1], seq_size))\n",
    "        self.X, self.Y = adBits(adj_edge, self.X, data_color)\n",
    "        self.Y = updateLabelBits(self.X, self.Y)\n",
    "        self.Y = np.eye(101, dtype='float32')[self.Y]\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32).to(device)\n",
    "        self.Y = torch.tensor(self.Y, dtype=torch.float32).to(device)\n",
    "        print(\"X shape: \", self.X.shape, \", Y shape: \", self.Y.shape)\n",
    "        # print(self.Y[38, 0:4, 0:6])\n",
    "        # X shape:  torch.Size([8158, 100, 100]) , Y shape:  torch.Size([8158, 100, 101])\n",
    "        # tensor([[0., 1., 0., 0., 0., 0.],\n",
    "        #         [0., 0., 1., 0., 0., 0.],\n",
    "        #         [0., 0., 0., 1., 0., 0.],\n",
    "        #         [0., 1., 0., 0., 0., 0.]], device='cuda:0')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ig = torch.nn.functional.normalize(self.X[idx], dim=1) # interference graph\n",
    "        label = self.Y[idx]\n",
    "        return ig, label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([8158, 100, 100]) , Y shape:  torch.Size([8158, 100, 101])\n",
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fcc58177970>, <torch.utils.data.dataloader.DataLoader object at 0x7fcc2478e370>)\n",
      "Length of train dataloader: 102 batches of 64\n",
      "Length of test dataloader: 26 batches of 64\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "full_dataset = CustomDataset(\n",
    "    img_dir=\"train_data.csv\",\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, \n",
    "                                                      [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        y = self.module(x_reshape)\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DLRegAlloc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm_1 = nn.LSTM(input_size=100, hidden_size=512, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.lstm_2 = nn.LSTM(input_size=1024, hidden_size=256, \n",
    "                              batch_first=True,bidirectional=True)\n",
    "        self.lstm_3 = nn.LSTM(input_size=512, hidden_size=128, \n",
    "                              batch_first=True,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.fc = nn.Linear(in_features=256, out_features=101)\n",
    "        # self.softmax = nn.Softmax(dim=2)\n",
    "        # self.time_distributed = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"----: \", x.shape)\n",
    "        x, _ = self.lstm_1(x)\n",
    "        # print(\"lstm1 shape: \", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, _ = self.lstm_2(x)\n",
    "        # print(\"lstm2 shape: \", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, _ = self.lstm_3(x)\n",
    "        # print(\"lstm3 shape: \", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        # print(\"fc shape: \", x.shape)\n",
    "        # x = self.softmax(x)\n",
    "        # x = self.time_distributed(x)\n",
    "        # print(\"time_distributed shape: \", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = DLRegAlloc().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = next(iter(train_dataloader))\n",
    "# test_x.size(-1)\n",
    "test_pred_logits = model(test_x)\n",
    "test_pred = torch.softmax(test_pred_logits, dim=2)\n",
    "# test_x.shape, test_y.shape, test_x[0, 0:6, 0:6], \\\n",
    "#     test_y[0, 0:6, 0:6], test_pred.shape, test_pred[0, 0:6, 0:6], \\\n",
    "#         test_pred.argmax(dim=2)\n",
    "test_y[0, 0:30, 0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "# , weight_decay=1e-4\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / (len(y_pred) * len(y_pred[0]))) * 100\n",
    "    # print(\"acc y_true shape: \", y_pred.shape)\n",
    "    # print(\"acc y_true shape: \", y_true.shape)\n",
    "    # print(\"corret: \", correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device,\n",
    "               train_loss_values=None,\n",
    "               train_acc_values=None,\n",
    "               tracking=False\n",
    "               ):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    # model.to(device)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # +++++++\n",
    "        # y_pred shape:  torch.Size([32, 100, 101])\n",
    "        # y shape:  torch.Size([32, 100, 101])\n",
    "        y_pred_logits = model(X)\n",
    "        y_pred = torch.softmax(y_pred_logits, dim=2)\n",
    "        loss = loss_fn(y_pred_logits.permute(0, 2, 1), y.permute(0, 2, 1))\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y.argmax(dim=2), y_pred=y_pred.argmax(dim=2))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    if tracking:\n",
    "        train_loss_values.append(train_loss.clone().detach().cpu().numpy())\n",
    "        train_acc_values.append(train_acc)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module, \n",
    "              accuracy_fn,\n",
    "              device: torch.device = device,\n",
    "              test_loss_values=None,\n",
    "              test_acc_values=None,\n",
    "              tracking=False):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in data_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            test_pred_logits = model(X_test)\n",
    "            test_pred = torch.softmax(test_pred_logits, dim=2)\n",
    "            test_loss += loss_fn(test_pred_logits.permute(0, 2, 1), \n",
    "                                 y_test.permute(0, 2, 1))\n",
    "            test_acc += accuracy_fn(y_true=y_test.argmax(dim=2), \n",
    "                                    y_pred=test_pred.argmax(dim=2))\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader) \n",
    "        if tracking:\n",
    "            test_loss_values.append(test_loss.clone().detach().cpu().numpy())\n",
    "            test_acc_values.append(test_acc)\n",
    "        print(f\"Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe1 = torch.randint(0, 3, (3, 2, 3))\n",
    "qwe2 = torch.randint(0, 3, (3, 2, 3))\n",
    "correct = torch.eq(qwe1, qwe2).sum().item()\n",
    "qwe1[0:2].shape\n",
    "# qwe1, qwe2, correct, qwe1.argmax(dim=1).shape, qwe1.squeeze(dim=0).shape, qwe1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850ece8d8f094e288ef08a94a400df94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.67419 | Train acc: 57.92%\n",
      "Test loss: 1.19956, Test acc: 61.70%\n",
      "\n",
      "Epoch: 1\n",
      "------\n",
      "Train loss: 1.05653 | Train acc: 63.84%\n",
      "Test loss: 0.98650, Test acc: 63.05%\n",
      "\n",
      "Epoch: 2\n",
      "------\n",
      "Train loss: 0.90363 | Train acc: 65.25%\n",
      "Test loss: 0.90775, Test acc: 64.76%\n",
      "\n",
      "Epoch: 3\n",
      "------\n",
      "Train loss: 0.85428 | Train acc: 66.37%\n",
      "Test loss: 0.87815, Test acc: 64.97%\n",
      "\n",
      "Epoch: 4\n",
      "------\n",
      "Train loss: 0.83300 | Train acc: 66.94%\n",
      "Test loss: 0.87207, Test acc: 65.16%\n",
      "\n",
      "Epoch: 5\n",
      "------\n",
      "Train loss: 0.82491 | Train acc: 67.20%\n",
      "Test loss: 0.86128, Test acc: 66.01%\n",
      "\n",
      "Epoch: 6\n",
      "------\n",
      "Train loss: 0.81766 | Train acc: 67.38%\n",
      "Test loss: 0.84957, Test acc: 65.99%\n",
      "\n",
      "Epoch: 7\n",
      "------\n",
      "Train loss: 0.81339 | Train acc: 67.51%\n",
      "Test loss: 0.84065, Test acc: 66.35%\n",
      "\n",
      "Epoch: 8\n",
      "------\n",
      "Train loss: 0.81339 | Train acc: 67.53%\n",
      "Test loss: 0.84945, Test acc: 65.98%\n",
      "\n",
      "Epoch: 9\n",
      "------\n",
      "Train loss: 0.80738 | Train acc: 67.68%\n",
      "Test loss: 0.83578, Test acc: 66.44%\n",
      "\n",
      "Epoch: 10\n",
      "------\n",
      "Train loss: 0.80035 | Train acc: 67.82%\n",
      "Test loss: 0.83071, Test acc: 66.45%\n",
      "\n",
      "Epoch: 11\n",
      "------\n",
      "Train loss: 0.79470 | Train acc: 68.02%\n",
      "Test loss: 0.82716, Test acc: 66.80%\n",
      "\n",
      "Epoch: 12\n",
      "------\n",
      "Train loss: 0.78962 | Train acc: 68.18%\n",
      "Test loss: 0.82858, Test acc: 66.62%\n",
      "\n",
      "Epoch: 13\n",
      "------\n",
      "Train loss: 0.78422 | Train acc: 68.32%\n",
      "Test loss: 0.81878, Test acc: 66.97%\n",
      "\n",
      "Epoch: 14\n",
      "------\n",
      "Train loss: 0.78213 | Train acc: 68.29%\n",
      "Test loss: 0.81749, Test acc: 66.88%\n",
      "\n",
      "Epoch: 15\n",
      "------\n",
      "Train loss: 0.77759 | Train acc: 68.38%\n",
      "Test loss: 0.81093, Test acc: 67.11%\n",
      "\n",
      "Epoch: 16\n",
      "------\n",
      "Train loss: 0.77815 | Train acc: 68.39%\n",
      "Test loss: 0.81175, Test acc: 67.13%\n",
      "\n",
      "Epoch: 17\n",
      "------\n",
      "Train loss: 0.77620 | Train acc: 68.45%\n",
      "Test loss: 0.80664, Test acc: 67.20%\n",
      "\n",
      "Epoch: 18\n",
      "------\n",
      "Train loss: 0.77191 | Train acc: 68.55%\n",
      "Test loss: 0.80651, Test acc: 67.24%\n",
      "\n",
      "Epoch: 19\n",
      "------\n",
      "Train loss: 0.76887 | Train acc: 68.66%\n",
      "Test loss: 0.80916, Test acc: 67.04%\n",
      "\n",
      "Epoch: 20\n",
      "------\n",
      "Train loss: 0.76858 | Train acc: 68.71%\n",
      "Test loss: 0.81653, Test acc: 66.70%\n",
      "\n",
      "Epoch: 21\n",
      "------\n",
      "Train loss: 0.76445 | Train acc: 68.76%\n",
      "Test loss: 0.80277, Test acc: 67.21%\n",
      "\n",
      "Epoch: 22\n",
      "------\n",
      "Train loss: 0.76193 | Train acc: 68.82%\n",
      "Test loss: 0.80255, Test acc: 67.36%\n",
      "\n",
      "Epoch: 23\n",
      "------\n",
      "Train loss: 0.76026 | Train acc: 68.92%\n",
      "Test loss: 0.80355, Test acc: 67.12%\n",
      "\n",
      "Epoch: 24\n",
      "------\n",
      "Train loss: 0.75959 | Train acc: 68.89%\n",
      "Test loss: 0.79789, Test acc: 67.49%\n",
      "\n",
      "Epoch: 25\n",
      "------\n",
      "Train loss: 0.75671 | Train acc: 68.97%\n",
      "Test loss: 0.79554, Test acc: 67.38%\n",
      "\n",
      "Epoch: 26\n",
      "------\n",
      "Train loss: 0.75647 | Train acc: 69.06%\n",
      "Test loss: 0.79494, Test acc: 67.52%\n",
      "\n",
      "Epoch: 27\n",
      "------\n",
      "Train loss: 0.75520 | Train acc: 69.07%\n",
      "Test loss: 0.82254, Test acc: 67.15%\n",
      "\n",
      "Epoch: 28\n",
      "------\n",
      "Train loss: 0.75659 | Train acc: 69.07%\n",
      "Test loss: 0.79244, Test acc: 67.63%\n",
      "\n",
      "Epoch: 29\n",
      "------\n",
      "Train loss: 0.75037 | Train acc: 69.22%\n",
      "Test loss: 0.78948, Test acc: 67.58%\n",
      "\n",
      "Epoch: 30\n",
      "------\n",
      "Train loss: 0.75077 | Train acc: 69.20%\n",
      "Test loss: 0.79409, Test acc: 67.37%\n",
      "\n",
      "Epoch: 31\n",
      "------\n",
      "Train loss: 0.74701 | Train acc: 69.30%\n",
      "Test loss: 0.79157, Test acc: 67.45%\n",
      "\n",
      "Epoch: 32\n",
      "------\n",
      "Train loss: 0.74520 | Train acc: 69.33%\n",
      "Test loss: 0.78870, Test acc: 67.55%\n",
      "\n",
      "Epoch: 33\n",
      "------\n",
      "Train loss: 0.74401 | Train acc: 69.43%\n",
      "Test loss: 0.78747, Test acc: 67.43%\n",
      "\n",
      "Epoch: 34\n",
      "------\n",
      "Train loss: 0.74176 | Train acc: 69.47%\n",
      "Test loss: 0.78719, Test acc: 67.41%\n",
      "\n",
      "Epoch: 35\n",
      "------\n",
      "Train loss: 0.74039 | Train acc: 69.56%\n",
      "Test loss: 0.78748, Test acc: 67.61%\n",
      "\n",
      "Epoch: 36\n",
      "------\n",
      "Train loss: 0.73785 | Train acc: 69.63%\n",
      "Test loss: 0.78723, Test acc: 67.43%\n",
      "\n",
      "Epoch: 37\n",
      "------\n",
      "Train loss: 0.73702 | Train acc: 69.66%\n",
      "Test loss: 0.78595, Test acc: 67.55%\n",
      "\n",
      "Epoch: 38\n",
      "------\n",
      "Train loss: 0.73580 | Train acc: 69.74%\n",
      "Test loss: 0.78393, Test acc: 67.72%\n",
      "\n",
      "Epoch: 39\n",
      "------\n",
      "Train loss: 0.73295 | Train acc: 69.81%\n",
      "Test loss: 0.78192, Test acc: 67.67%\n",
      "\n",
      "Epoch: 40\n",
      "------\n",
      "Train loss: 0.73139 | Train acc: 69.87%\n",
      "Test loss: 0.78314, Test acc: 67.69%\n",
      "\n",
      "Epoch: 41\n",
      "------\n",
      "Train loss: 0.73111 | Train acc: 69.92%\n",
      "Test loss: 0.78381, Test acc: 67.67%\n",
      "\n",
      "Epoch: 42\n",
      "------\n",
      "Train loss: 0.72776 | Train acc: 70.06%\n",
      "Test loss: 0.78186, Test acc: 67.51%\n",
      "\n",
      "Epoch: 43\n",
      "------\n",
      "Train loss: 0.72743 | Train acc: 70.10%\n",
      "Test loss: 0.78670, Test acc: 67.57%\n",
      "\n",
      "Epoch: 44\n",
      "------\n",
      "Train loss: 0.72568 | Train acc: 70.16%\n",
      "Test loss: 0.78693, Test acc: 67.60%\n",
      "\n",
      "Epoch: 45\n",
      "------\n",
      "Train loss: 0.72287 | Train acc: 70.30%\n",
      "Test loss: 0.78331, Test acc: 67.66%\n",
      "\n",
      "Epoch: 46\n",
      "------\n",
      "Train loss: 0.72059 | Train acc: 70.41%\n",
      "Test loss: 0.78269, Test acc: 67.67%\n",
      "\n",
      "Epoch: 47\n",
      "------\n",
      "Train loss: 0.71832 | Train acc: 70.58%\n",
      "Test loss: 0.78696, Test acc: 67.57%\n",
      "\n",
      "Epoch: 48\n",
      "------\n",
      "Train loss: 0.71569 | Train acc: 70.73%\n",
      "Test loss: 0.78615, Test acc: 67.60%\n",
      "\n",
      "Epoch: 49\n",
      "------\n",
      "Train loss: 0.71350 | Train acc: 70.83%\n",
      "Test loss: 0.78599, Test acc: 67.66%\n",
      "\n",
      "Epoch: 50\n",
      "------\n",
      "Train loss: 0.71127 | Train acc: 70.97%\n",
      "Test loss: 0.78877, Test acc: 67.47%\n",
      "\n",
      "Epoch: 51\n",
      "------\n",
      "Train loss: 0.70833 | Train acc: 71.12%\n",
      "Test loss: 0.79155, Test acc: 67.49%\n",
      "\n",
      "Epoch: 52\n",
      "------\n",
      "Train loss: 0.70575 | Train acc: 71.33%\n",
      "Test loss: 0.79184, Test acc: 67.43%\n",
      "\n",
      "Epoch: 53\n",
      "------\n",
      "Train loss: 0.70146 | Train acc: 71.56%\n",
      "Test loss: 0.79593, Test acc: 67.50%\n",
      "\n",
      "Epoch: 54\n",
      "------\n",
      "Train loss: 0.69731 | Train acc: 71.78%\n",
      "Test loss: 0.79826, Test acc: 67.46%\n",
      "\n",
      "Epoch: 55\n",
      "------\n",
      "Train loss: 0.69293 | Train acc: 72.06%\n",
      "Test loss: 0.80226, Test acc: 67.38%\n",
      "\n",
      "Epoch: 56\n",
      "------\n",
      "Train loss: 0.68722 | Train acc: 72.38%\n",
      "Test loss: 0.80779, Test acc: 67.46%\n",
      "\n",
      "Epoch: 57\n",
      "------\n",
      "Train loss: 0.68191 | Train acc: 72.66%\n",
      "Test loss: 0.81066, Test acc: 67.13%\n",
      "\n",
      "Epoch: 58\n",
      "------\n",
      "Train loss: 0.67731 | Train acc: 72.97%\n",
      "Test loss: 0.81982, Test acc: 67.34%\n",
      "\n",
      "Epoch: 59\n",
      "------\n",
      "Train loss: 0.66962 | Train acc: 73.38%\n",
      "Test loss: 0.81832, Test acc: 67.37%\n",
      "\n",
      "Epoch: 60\n",
      "------\n",
      "Train loss: 0.66269 | Train acc: 73.81%\n",
      "Test loss: 0.83257, Test acc: 67.16%\n",
      "\n",
      "Epoch: 61\n",
      "------\n",
      "Train loss: 0.65536 | Train acc: 74.22%\n",
      "Test loss: 0.84027, Test acc: 67.26%\n",
      "\n",
      "Epoch: 62\n",
      "------\n",
      "Train loss: 0.64643 | Train acc: 74.69%\n",
      "Test loss: 0.85290, Test acc: 67.26%\n",
      "\n",
      "Epoch: 63\n",
      "------\n",
      "Train loss: 0.63722 | Train acc: 75.21%\n",
      "Test loss: 0.85441, Test acc: 67.19%\n",
      "\n",
      "Epoch: 64\n",
      "------\n",
      "Train loss: 0.62664 | Train acc: 75.77%\n",
      "Test loss: 0.87671, Test acc: 67.17%\n",
      "\n",
      "Epoch: 65\n",
      "------\n",
      "Train loss: 0.61567 | Train acc: 76.34%\n",
      "Test loss: 0.88691, Test acc: 67.13%\n",
      "\n",
      "Epoch: 66\n",
      "------\n",
      "Train loss: 0.60296 | Train acc: 76.98%\n",
      "Test loss: 0.90116, Test acc: 67.11%\n",
      "\n",
      "Epoch: 67\n",
      "------\n",
      "Train loss: 0.59175 | Train acc: 77.54%\n",
      "Test loss: 0.92390, Test acc: 67.06%\n",
      "\n",
      "Epoch: 68\n",
      "------\n",
      "Train loss: 0.57894 | Train acc: 78.15%\n",
      "Test loss: 0.93874, Test acc: 67.07%\n",
      "\n",
      "Epoch: 69\n",
      "------\n",
      "Train loss: 0.56500 | Train acc: 78.84%\n",
      "Test loss: 0.95348, Test acc: 67.02%\n",
      "\n",
      "Epoch: 70\n",
      "------\n",
      "Train loss: 0.54992 | Train acc: 79.52%\n",
      "Test loss: 0.98933, Test acc: 66.98%\n",
      "\n",
      "Epoch: 71\n",
      "------\n",
      "Train loss: 0.53325 | Train acc: 80.26%\n",
      "Test loss: 1.02358, Test acc: 67.03%\n",
      "\n",
      "Epoch: 72\n",
      "------\n",
      "Train loss: 0.51867 | Train acc: 80.87%\n",
      "Test loss: 1.03469, Test acc: 67.01%\n",
      "\n",
      "Epoch: 73\n",
      "------\n",
      "Train loss: 0.50399 | Train acc: 81.54%\n",
      "Test loss: 1.06688, Test acc: 66.94%\n",
      "\n",
      "Epoch: 74\n",
      "------\n",
      "Train loss: 0.48690 | Train acc: 82.27%\n",
      "Test loss: 1.09564, Test acc: 66.97%\n",
      "\n",
      "Epoch: 75\n",
      "------\n",
      "Train loss: 0.47298 | Train acc: 82.84%\n",
      "Test loss: 1.12701, Test acc: 66.94%\n",
      "\n",
      "Epoch: 76\n",
      "------\n",
      "Train loss: 0.45326 | Train acc: 83.65%\n",
      "Test loss: 1.17729, Test acc: 66.95%\n",
      "\n",
      "Epoch: 77\n",
      "------\n",
      "Train loss: 0.43577 | Train acc: 84.37%\n",
      "Test loss: 1.20154, Test acc: 66.92%\n",
      "\n",
      "Epoch: 78\n",
      "------\n",
      "Train loss: 0.41990 | Train acc: 84.97%\n",
      "Test loss: 1.25013, Test acc: 66.89%\n",
      "\n",
      "Epoch: 79\n",
      "------\n",
      "Train loss: 0.40515 | Train acc: 85.56%\n",
      "Test loss: 1.28068, Test acc: 66.91%\n",
      "\n",
      "Epoch: 80\n",
      "------\n",
      "Train loss: 0.38875 | Train acc: 86.23%\n",
      "Test loss: 1.31861, Test acc: 66.85%\n",
      "\n",
      "Epoch: 81\n",
      "------\n",
      "Train loss: 0.37348 | Train acc: 86.74%\n",
      "Test loss: 1.36250, Test acc: 66.86%\n",
      "\n",
      "Epoch: 82\n",
      "------\n",
      "Train loss: 0.35505 | Train acc: 87.48%\n",
      "Test loss: 1.40549, Test acc: 66.82%\n",
      "\n",
      "Epoch: 83\n",
      "------\n",
      "Train loss: 0.34159 | Train acc: 88.00%\n",
      "Test loss: 1.44479, Test acc: 66.90%\n",
      "\n",
      "Epoch: 84\n",
      "------\n",
      "Train loss: 0.32506 | Train acc: 88.62%\n",
      "Test loss: 1.48764, Test acc: 66.82%\n",
      "\n",
      "Train time on cuda: 260.959 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "train_time_start_model_2 = timer()\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "test_loss_values = []\n",
    "test_acc_values = []\n",
    "epoch_count = []\n",
    "\n",
    "epochs = 85\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n------\")\n",
    "    tracking = False\n",
    "    if epoch % 10 == 0:\n",
    "        tracking = True\n",
    "        epoch_count.append(epoch)\n",
    "    train_step(model=model,\n",
    "               data_loader=train_dataloader,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               accuracy_fn=accuracy_fn,\n",
    "               device=device,\n",
    "               train_loss_values=train_loss_values,\n",
    "               train_acc_values=train_acc_values,\n",
    "               tracking=tracking)\n",
    "    test_step(data_loader=test_dataloader,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_fn,\n",
    "            device=device,\n",
    "            test_loss_values=test_loss_values,\n",
    "            test_acc_values=test_acc_values,\n",
    "            tracking=tracking)\n",
    "    # if early_stopper.early_stop(test_loss_values[-1]):\n",
    "    #     print(\"\\nearly stop\\n\")\n",
    "    #     break\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: torch_version_CROSSENTROPYLOSS_12_09_best_85.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_SAVE_PATH = \"torch_version_CROSSENTROPYLOSS_12_09_best_85.pth\"\n",
    "\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss & accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = DLRegAlloc().to(device)\n",
    "loaded_model.load_state_dict(torch.load(f=\"torch_version_CROSSENTROPYLOSS_12_09_best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/baidu.csv information:\n",
      "torch.Size([1, 100, 100])\n",
      "torch.Size([1, 100, 101])\n",
      "test/gkarate.csv information:\n",
      "torch.Size([1, 100, 100])\n",
      "torch.Size([1, 100, 101])\n",
      "\n",
      "------PREDICTING  test/baidu.csv -------\n",
      "\n",
      "Invalid edges percentage before color correction ->\n",
      "Total No of edges  91\n",
      "# of edges with invalid coloring  21\n",
      "Total percentage of edges with invalid colors  0.23076923076923078\n",
      "\n",
      "Colors list and Chromatic number predicted by the model ->\n",
      "Colors list of graph  0  is  \n",
      " [1, 2, 3, 1, 1, 5, 3, 3, 4, 6, 6, 5, 6, 1, 2, 2, 1, 2, 2, 3, 3, 3, 2, 1, 5, 4, 6, 1, 1, 7, 7, 2, 2, 3, 5, 6, 2, 5, 1, 3, 4, 4, 4, 4, 2, 3, 1, 9, 2, 2, 2, 5, 3, 6, 6, 6, 2, 6, 6, 10]\n",
      "Chromatic number of graph  0  is   9\n",
      "\n",
      "Apply color correction ->\n",
      "\n",
      "Colors list and Chromatic number following color correction ->\n",
      "Colors list of graph  0  is  \n",
      " [1, 2, 3, 1, 1, 5, 3, 3, 4, 6, 6, 5, 6, 2, 2, 2, 1, 1, 2, 3, 3, 3, 2, 1, 5, 4, 6, 1, 3, 1, 7, 1, 2, 3, 5, 6, 2, 5, 1, 1, 4, 1, 4, 1, 2, 3, 2, 9, 2, 1, 2, 5, 1, 6, 1, 6, 2, 1, 6, 10]\n",
      "Chromatic number of graph  0  is   9\n",
      "\n",
      "Invalid edges percentage after color correction ->\n",
      "Total No of edges  91\n",
      "# of edges with invalid coloring  0\n",
      "Total percentage of edges with invalid colors  0.0\n",
      "[['baidu.csv', 0, 9, 9]]\n",
      "\n",
      "------PREDICTING  test/gkarate.csv -------\n",
      "\n",
      "Invalid edges percentage before color correction ->\n",
      "Total No of edges  79\n",
      "# of edges with invalid coloring  10\n",
      "Total percentage of edges with invalid colors  0.12658227848101267\n",
      "\n",
      "Colors list and Chromatic number predicted by the model ->\n",
      "Colors list of graph  0  is  \n",
      " [1, 2, 3, 1, 4, 4, 6, 7, 7, 2, 7, 6, 6, 7, 2, 1, 2, 6, 2, 6, 2, 1, 2, 1, 2, 2, 3, 1, 1, 3, 1, 7, 1, 3]\n",
      "Chromatic number of graph  0  is   6\n",
      "\n",
      "Apply color correction ->\n",
      "\n",
      "Colors list and Chromatic number following color correction ->\n",
      "Colors list of graph  0  is  \n",
      " [1, 2, 3, 4, 4, 4, 6, 7, 7, 2, 7, 6, 6, 7, 2, 2, 2, 6, 2, 6, 2, 3, 2, 2, 2, 3, 1, 1, 1, 3, 1, 7, 4, 5]\n",
      "Chromatic number of graph  0  is   7\n",
      "\n",
      "Invalid edges percentage after color correction ->\n",
      "Total No of edges  79\n",
      "# of edges with invalid coloring  0\n",
      "Total percentage of edges with invalid colors  0.0\n",
      "[['gkarate.csv', 0, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "test_csvs = [\"test/baidu.csv\", \"test/gkarate.csv\"]\n",
    "\n",
    "x_pred_list = []\n",
    "y_pred_list = []\n",
    "seq_size = 100\n",
    "\n",
    "for i,csv in enumerate(test_csvs):\n",
    "    print(csv, \"information:\")\n",
    "    \n",
    "    seq = pd.read_csv(csv, header=None, low_memory=False)\n",
    "    columns = seq.columns.tolist()\n",
    "    #get the adj edges\n",
    "    adj_edge = np.array(seq[columns[1:2*seq_size+1]])\n",
    "    #get the colors\n",
    "    data_color = np.array(seq[columns[2*seq_size+1:3*seq_size+1]])\n",
    "    X = np.zeros((adj_edge.shape[0], data_color.shape[1], seq_size))\n",
    "    X, Y = adBits(adj_edge, X, data_color)\n",
    "    Y = updateLabelBits(X, Y)\n",
    "    Y = np.eye(101, dtype='float32')[Y]\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "    x_pred_list.append(X)\n",
    "    y_pred_list.append(Y)\n",
    "\n",
    "    print(x_pred_list[i].shape)\n",
    "    print(y_pred_list[i].shape)  \n",
    "\n",
    "import pickle\n",
    "\n",
    "for i,csv in enumerate(test_csvs):\n",
    "    print('\\n------PREDICTING ',csv,'-------')\n",
    "    loaded_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        predicted = loaded_model(x_pred_list[i]) # torch.Size([1, 100, 101])\n",
    "        predicted = torch.softmax(predicted, dim=2)\n",
    "        # print(\"softmaxpredicted shape: \", predicted.shape)\n",
    "        predicted = predicted.clone().detach().cpu().numpy() # (100, 101)\n",
    "        # print(predicted.shape) # (1, 100, 101)\n",
    "        # print(np.argmax(predicted, axis=2)) # (1,100)\n",
    "\n",
    "        x_pred = x_pred_list[i].clone().detach().cpu() # torch.Size([1, 100, 100])\n",
    "        print('\\nInvalid edges percentage before color correction ->')\n",
    "        post_process(np.asarray(x_pred.tolist()), predicted)\n",
    "        \n",
    "        print('\\nColors list and Chromatic number predicted by the model ->')\n",
    "        colors_list_list_before_correction = post_process_chromatic(np.asarray(x_pred), predicted)\n",
    "        print('\\nApply color correction ->')\n",
    "        predicted = post_process_correction(np.asarray(x_pred), predicted, colors_list_list_before_correction)\n",
    "        print('\\nColors list and Chromatic number following color correction ->')\n",
    "        colors_list_list_after_correction = post_process_chromatic(np.asarray(x_pred), predicted)\n",
    "        print('\\nInvalid edges percentage after color correction ->')\n",
    "        post_process(np.asarray(x_pred), predicted)\n",
    "        results = create_csv_rows(csv.rsplit('/',-1)[-1], colors_list_list_before_correction, colors_list_list_after_correction)\n",
    "        print(results)\n",
    "        with open(\"results\", \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(colors_list_list_after_correction, fp)\n",
    "# import csv\n",
    "# with open(\"Result/pytorch_graph_coloring_result.csv\", 'w') as csvfile:\n",
    "#     # creating a csv writer object\n",
    "#     csvwriter = csv.writer(csvfile)      \n",
    "#     # writing the data rows\n",
    "#     csvwriter.writerows(csv_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 4, 4, 6, 7, 7, 2, 7, 6, 6, 7, 2, 2, 2, 6, 2, 6, 2, 3, 2, 2, 2, 3, 1, 1, 1, 3, 1, 7, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"results\", \"rb\") as fp:\n",
    "    color_list = pickle.load(fp)\n",
    "print(color_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs583",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
