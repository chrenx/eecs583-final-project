{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.__version__\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils and Input & Output Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== Utils =========================================\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    if ('val_categorical_accuracy' in history.history.keys()):\n",
    "        plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    if ('val_loss' in history.history.keys()):\n",
    "        plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "def adBits(x, x2, y, seqsize=100):\n",
    "   for i in range(x.shape[0]):\n",
    "      #print(' for each train sample  ... ', i)\n",
    "      loop = seqsize\n",
    "      j = 0\n",
    "      for loop in range(seqsize):     \n",
    "          #print(' for each node ... ', j)\n",
    "          adBits_1 = int(x[i][2*loop])\n",
    "          #Get the next LONG since each node now has 2 LONGs for adj edges\n",
    "          adBits_2 = int(x[i][2*loop+1])\n",
    "          # Uncomment below if you want invalid nodes to not feed to NN before any valid nodes\n",
    "          #if (adBits_1==0 and adBits_1==0 and loop!=seqsize-1):              \n",
    "          #    y[i][j] = y[i][loop+1] \n",
    "          #    continue\n",
    "          if (adBits_1):\n",
    "              #print adBits\n",
    "              for k in range(0,64):\n",
    "                  bit = adBits_1 & 1\n",
    "                  #print ( ' adBits&1 = ',  bit )\n",
    "                  # Uncomment below if you want new nodes to only have adjacency to earlier nodes\n",
    "                  if ( bit == 1):\n",
    "                  #if ( bit == 1 and k<j):\n",
    "                      x2[i][j][k] = 1\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  else:\n",
    "                      x2[i][j][k] = 0\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  adBits_1 >>= 1\n",
    "              x2[i][j][j] = 1 \n",
    "\n",
    "          if (adBits_2):   \n",
    "              #print adBits\n",
    "              for k in range(64,100):\n",
    "                  bit = adBits_2 & 1\n",
    "                  #print ( ' adBits&1 = ',  bit )\n",
    "                  if ( bit == 1):\n",
    "                  #if ( bit == 1 and k<j):\n",
    "                      x2[i][j][k] = 1\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  else:\n",
    "                      x2[i][j][k] = 0\n",
    "                      #print x2[i-1][j-1][k-1]\n",
    "                  adBits_2 >>= 1\n",
    "              x2[i][j][j] = 1\n",
    "          j = j+1\n",
    "   return x2, y\n",
    "\n",
    "def updateLabelBits(x, y, seqsize=100):\n",
    "    for i in range(x.shape[0]):\n",
    "        #print(' for each train sample  ... ', i)\n",
    "        label_dict = {}\n",
    "        label_num = 1\n",
    "        for j in range(seqsize):\n",
    "            if (x[i][j][j] == 0):\n",
    "                y[i][j] = 0\n",
    "            else:\n",
    "                if y[i][j] in label_dict:\n",
    "                    y[i][j] = label_dict[y[i][j]]\n",
    "                else:\n",
    "                    label_dict[y[i][j]] = label_num\n",
    "                    y[i][j] = label_num\n",
    "                    label_num = label_num + 1\n",
    "    return y\n",
    "\n",
    "# ============================== Post Process ==================================\n",
    "\n",
    "csv_rows = []\n",
    "\n",
    "def post_process (x2_pred, predicted, seqsize=100):\n",
    "    #Calculate the number of edges which will require correction\n",
    "    invCols = 0\n",
    "    edges = 0\n",
    "    for i in range(x2_pred.shape[0]):\n",
    "        for j in range(seqsize): # row\n",
    "            for k in range(j): # col\n",
    "                adj = x2_pred[i][j][k]\n",
    "                if (adj == 1):\n",
    "                    edges += 1\n",
    "                    if (np.argmax(predicted[i][j]) == np.argmax(predicted[i][k])):\n",
    "                        invCols += 1\n",
    "                        \n",
    "    print('Total No of edges ', edges)\n",
    "    print('# of edges with invalid coloring ', invCols)\n",
    "    print('Total percentage of edges with invalid colors ', invCols/edges)\n",
    "\n",
    "def post_process_chromatic (x2_pred, predicted, seqsize=100):  \n",
    "    colors_list_list = []\n",
    "    for i in range(x2_pred.shape[0]):\n",
    "        colors_list = []\n",
    "        for j in range(seqsize):\n",
    "            # Valid nodes will have below set to 1 so check the color \n",
    "            # assignment of those nodes only\n",
    "            if (x2_pred[i][j][j] != 0):\n",
    "                colors_list.append(np.argmax(predicted[i][j]))\n",
    "        print('Colors list of graph ', i, ' is  \\n', colors_list)\n",
    "        chromatic_number = len(set(colors_list))\n",
    "        print('Chromatic number of graph ', i, ' is  ', chromatic_number)\n",
    "        colors_list_list.append(colors_list)\n",
    "    return colors_list_list\n",
    "\n",
    "def create_csv_rows (graph_name, colors_list_list_before_correction, \n",
    "                     colors_list_list_after_correction):    \n",
    "    for i in range(len(colors_list_list_before_correction)):\n",
    "        row = [graph_name, i, len(set(colors_list_list_before_correction[i])), \n",
    "               len(set(colors_list_list_after_correction[i]))]        \n",
    "        csv_rows.append(row)\n",
    "\n",
    "def post_process_correction (x2_pred, predicted, colors_list_list, seqsize=100): \n",
    "  totInvCols = 0\n",
    "  totEdges = 0\n",
    "\n",
    "  for i in range(x2_pred.shape[0]):\n",
    "      #maxcol = max(xpredicted[i])\n",
    "      maxcol = max(colors_list_list[i])\n",
    "      #print(maxcol)\n",
    "      #mcol = maxcol[0]\n",
    "      maxorigcol = maxcol\n",
    "      mcolnew = maxcol\n",
    "      #print('Maxcol = ',maxcol[0])\n",
    "      #print(' ... FOR SAMPLE  ... ', i)\n",
    "      invCols = 0\n",
    "      edges = 0;\n",
    "      newCol = 500\n",
    "\n",
    "      for j in range(seqsize):\n",
    "          #print(' ... ... FOR EACH NODE ... ...', j)\n",
    "          for k in range(j):\n",
    "              #print(' ... ... ... for each adjacency  ... ... ...', k)\n",
    "              adj = x2_pred[i][j][k]\n",
    "              #There is an edge\n",
    "              if ( adj == 1 ):\n",
    "                  edges += 1\n",
    "                  if ( np.argmax(predicted[i][j]) == np.argmax(predicted[i][k]) ):                   \n",
    "                      col_j = np.argmax(predicted[i][j])\n",
    "                      col_k = np.argmax(predicted[i][k])\n",
    "                      invCols += 1\n",
    "\n",
    "                      #Check whether we can give one of the existing colors\n",
    "                      foundfinalcol = 0\n",
    "                      for  y in range(1,maxcol+1):\n",
    "                          #print('Check for COLOR NO ... ', y)\n",
    "                          if ( foundfinalcol == 1 ) :\n",
    "                              #print('FOUND COLOR ALREADY  ... leave the loop')\n",
    "                              break\n",
    "\n",
    "                          foundcol = 0\n",
    "                          #Check the adjacent nodes of j\n",
    "                          #for  z in range(j):\n",
    "                          for z in range(seqsize):\n",
    "                              if j!=z:\n",
    "                                  if  (   ((x2_pred[i][j][z] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                      or  ((x2_pred[i][z][j] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                      ):\n",
    "                                      #print('[1] Adjacent node ... from ',j, '-->', z, 'color = ',xpredicted[i][z][0] )\n",
    "                                      foundcol = 1\n",
    "                                      #print('[1] Found Color ', y, ' for node ', z, 'from node ', j )\n",
    "                                      break\n",
    "\n",
    "                          #Finished checking the adjacent nodes of j\n",
    "                          #Color y is not used by any of j's neighbours\n",
    "                          #print('[1] Finished Checking the adjacent node of ... ',j,' ... foundcol = ',foundcol)\n",
    "                          if ( foundcol == 0 ) :\n",
    "                              #assign any prediction > 1\n",
    "                              predicted[i][j][y] = 2\n",
    "                              #print('[1] Reuse color ', y, ' for node ', j)\n",
    "                              foundfinalcol = 1\n",
    "\n",
    "                          else :\n",
    "                              foundcol = 0                                                            \n",
    "                              #Check the adjacent nodes of k\n",
    "                              for z in range(seqsize):\n",
    "                                  if k!=z:\n",
    "                                      if  (   ((x2_pred[i][k][z] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                          or  ((x2_pred[i][z][k] == 1) and (np.argmax(predicted[i][z]) == y))\n",
    "                                          ):\n",
    "                                          #print('[1] Adjacent node ... from ',j, '-->', z, 'color = ',xpredicted[i][z][0] )\n",
    "                                          foundcol = 1\n",
    "                                          #print('[1] Found Color ', y, ' for node ', z, 'from node ', j )\n",
    "                                          break\n",
    "                              #Color y is not used by any of k's neighbours\n",
    "                              if ( foundcol == 0 ) :\n",
    "                                  #assign any prediction > 1\n",
    "                                  predicted[i][k][y] = 2\n",
    "                                  #print('[2] Reuse color ', y, ' for node ', k )\n",
    "                                  foundfinalcol = 1\n",
    "\n",
    "                      # Could not color using an existing color\n",
    "                      # Get a new color from 500 onwards OR use from the new 500 color number series\n",
    "                      if ( foundfinalcol == 0 ) :\n",
    "                           #newCol += 1\n",
    "                           mcolnew += 1\n",
    "                           #assign any prediction > 1\n",
    "                           predicted[i][k][mcolnew] = 2\n",
    "                           maxcol +=1\n",
    "                           #print('Use new color ', mcolnew, ' for node ', k)\n",
    "\n",
    "  return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape 1:  (8158, 200) (8158, 100)\n",
      "shape 2:  (8158, 100, 100)\n",
      "shape 3:  (8158, 100, 100) (8158, 100)\n"
     ]
    }
   ],
   "source": [
    "seq_size = 100\n",
    "seq = pd.read_csv(\"train_data.csv\",header=None,low_memory=False)\n",
    "columns = seq.columns.tolist()\n",
    "\n",
    "#get the adj edges\n",
    "adj_edge = np.array(seq[columns[1:2*seq_size+1]])\n",
    "#get the colors\n",
    "data_color = np.array(seq[columns[2*seq_size+1:3*seq_size+1]])\n",
    "print(\"shape 1: \", adj_edge.shape, data_color.shape)\n",
    "\n",
    "X = np.zeros((adj_edge.shape[0], data_color.shape[1], seq_size))\n",
    "print(\"shape 2: \", X.shape)\n",
    "\n",
    "X, Y = adBits(adj_edge, X, data_color)\n",
    "Y = updateLabelBits(X, Y)\n",
    "\n",
    "\n",
    "print(\"shape 3: \", X.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8158, 100, 100]), torch.Size([8158, 100, 101]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.eye(101, dtype='float32')[Y]\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6526, 100, 100]),\n",
       " torch.Size([6526, 100, 101]),\n",
       " torch.Size([1632, 100, 100]),\n",
       " torch.Size([1632, 100, 101]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "x_train, x_test= x_train.to(device), x_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 11:26:41.668696: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-07 11:26:41.902653: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-07 11:26:41.902807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-07 11:26:41.941682: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-07 11:26:42.023480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-07 11:26:42.836688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8158, 100, 100)\n",
      "(8158, 100, 101)\n"
     ]
    }
   ],
   "source": [
    "seqsize = 100\n",
    "\n",
    "eseq1 = pd.read_csv(\"train_data.csv\",header=None,low_memory=False)\n",
    "\n",
    "eseq  = eseq1\n",
    "\n",
    "columns = eseq.columns.tolist()\n",
    "#2 entries for each node's adjacency \n",
    "edata = eseq[columns[1:3*seqsize+1]]\n",
    "#get the adj edges\n",
    "edataadj = eseq[columns[1:2*seqsize+1]]\n",
    "#get the colors\n",
    "edatacols = eseq[columns[2*seqsize+1:3*seqsize+1]]\n",
    "#chromatic number\n",
    "ncols = eseq[columns[0:1]]\n",
    "\n",
    "#Now make x_train a 2-dim array\n",
    "#As we have read the adjacency row as bits\n",
    "#and we now want to make them into features\n",
    "#x_train and x2_train have different shapes as 2 LONGs are present in x_train\n",
    "#So use y_train.shape[1] instead of x_train.shape[1]\n",
    "#x2_train = np.zeros((x_train.shape[0],y_train.shape[1],128))\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train, y_train = np.array(edataadj), np.array(edatacols)\n",
    "x2_train = np.zeros((x_train.shape[0], y_train.shape[1], seqsize))\n",
    "x2_train, y_train = adBits(x_train, x2_train, y_train)\n",
    "y_train = updateLabelBits(x2_train, y_train)\n",
    "y2_train = to_categorical(y_train, num_classes=101)\n",
    "print(x2_train.shape)\n",
    "print(y2_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 0., 0.]]),\n",
       " array([ 6.,  0., 13.,  0., 11.,  0.]),\n",
       " array([1, 2, 3, 1, 0, 0]),\n",
       " array([[0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y2_train[0][0:3]\n",
    "# y_train.shape, y_train[38], y2_train[38][3]\n",
    "x2_train[38, 0:4, 0:6], x_train[38, 0:6], y_train[38, 0:6], y2_train[38, 0:4, 0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, seq_size=100, device=device):\n",
    "        seq = pd.read_csv(img_dir, header=None, low_memory=False)\n",
    "        columns = seq.columns.tolist()\n",
    "        #get the adj edges\n",
    "        adj_edge = np.array(seq[columns[1:2*seq_size+1]])\n",
    "        #get the colors\n",
    "        data_color = np.array(seq[columns[2*seq_size+1:3*seq_size+1]])\n",
    "        self.X = np.zeros((adj_edge.shape[0], data_color.shape[1], seq_size))\n",
    "        self.X, self.Y = adBits(adj_edge, self.X, data_color)\n",
    "        self.Y = updateLabelBits(self.X, self.Y)\n",
    "        self.Y = np.eye(101, dtype='float32')[self.Y]\n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32).to(device)\n",
    "        self.Y = torch.tensor(self.Y, dtype=torch.float32).to(device)\n",
    "        print(\"X shape: \", self.X.shape, \", Y shape: \", self.Y.shape)\n",
    "        # print(self.Y[38, 0:4, 0:6])\n",
    "        # X shape:  torch.Size([8158, 100, 100]) , Y shape:  torch.Size([8158, 100, 101])\n",
    "        # tensor([[0., 1., 0., 0., 0., 0.],\n",
    "        #         [0., 0., 1., 0., 0., 0.],\n",
    "        #         [0., 0., 0., 1., 0., 0.],\n",
    "        #         [0., 1., 0., 0., 0., 0.]], device='cuda:0')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ig = self.X[idx] # interference graph\n",
    "        label = self.Y[idx]\n",
    "        return ig, label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([8158, 100, 100]) , Y shape:  torch.Size([8158, 100, 101])\n",
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7f3421c8cb20>, <torch.utils.data.dataloader.DataLoader object at 0x7f3421c0db20>)\n",
      "Length of train dataloader: 204 batches of 32\n",
      "Length of test dataloader: 51 batches of 32\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "full_dataset = CustomDataset(\n",
    "    img_dir=\"train_data.csv\",\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, \n",
    "                                                      [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "        y = self.module(x_reshape)\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "        return y\n",
    "\n",
    "\n",
    "class DLRegAlloc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm_1 = nn.LSTM(input_size=100, hidden_size=512, \n",
    "                              batch_first=True, bidirectional=True)\n",
    "        self.lstm_2 = nn.LSTM(input_size=1024, hidden_size=256, \n",
    "                              batch_first=True,bidirectional=True)\n",
    "        self.lstm_3 = nn.LSTM(input_size=512, hidden_size=128, \n",
    "                              batch_first=True,bidirectional=True)\n",
    "        # self.lstm_1 = nn.LSTM(input_size=512, hidden_size=256, bidirectional=True)\n",
    "        # self.lstm_2 = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True)\n",
    "        # self.lstm_3 = nn.LSTM(input_size=128, hidden_size=64, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(in_features=256, out_features=101)\n",
    "        # self.softmax = nn.Softmax(dim=2)\n",
    "        # self.time_distributed = TimeDistributed(self.softmax, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(\"----: \", x.shape)\n",
    "        x, _ = self.lstm_1(x)\n",
    "        # print(\"lstm1 shape: \", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, _ = self.lstm_2(x)\n",
    "        # print(\"lstm2 shape: \", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, _ = self.lstm_3(x)\n",
    "        # print(\"lstm3 shape: \", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        # print(\"fc shape: \", x.shape)\n",
    "        # x = self.softmax(x)\n",
    "        # x = self.time_distributed(x)\n",
    "        # print(\"time_distributed shape: \", x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = DLRegAlloc().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 100, 100]),\n",
       " torch.Size([32, 100, 101]),\n",
       " tensor([[1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1., 1.]], device='cuda:0'),\n",
       " tensor([[0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0., 0.]], device='cuda:0'),\n",
       " torch.Size([32, 100, 101]),\n",
       " tensor([[0.0105, 0.0106, 0.0096, 0.0100, 0.0100, 0.0095],\n",
       "         [0.0107, 0.0108, 0.0094, 0.0098, 0.0100, 0.0092],\n",
       "         [0.0108, 0.0107, 0.0096, 0.0097, 0.0098, 0.0096],\n",
       "         [0.0107, 0.0108, 0.0095, 0.0095, 0.0100, 0.0097],\n",
       "         [0.0107, 0.0107, 0.0098, 0.0099, 0.0102, 0.0093],\n",
       "         [0.0107, 0.0109, 0.0095, 0.0098, 0.0101, 0.0095]], device='cuda:0',\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([[ 9,  9,  9,  ...,  9,  9,  9],\n",
       "         [ 9, 95,  9,  ...,  9,  9,  9],\n",
       "         [14,  1,  9,  ...,  9,  9,  9],\n",
       "         ...,\n",
       "         [ 9,  9,  1,  ...,  9,  9,  9],\n",
       "         [95,  0,  9,  ..., 65,  9,  9],\n",
       "         [ 9, 22,  9,  ...,  9,  9,  9]], device='cuda:0'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = next(iter(train_dataloader))\n",
    "# test_x.size(-1)\n",
    "test_pred_logits = model(test_x)\n",
    "test_pred = torch.softmax(test_pred_logits, dim=2)\n",
    "test_x.shape, test_y.shape, test_x[0, 0:6, 0:6], \\\n",
    "    test_y[0, 0:6, 0:6], test_pred.shape, test_pred[0, 0:6, 0:6], \\\n",
    "        test_pred.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / (len(y_pred) * len(y_pred[0]))) * 100\n",
    "    # print(\"acc y_true shape: \", y_pred.shape)\n",
    "    # print(\"acc y_true shape: \", y_true.shape)\n",
    "    # print(\"corret: \", correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device,\n",
    "               train_loss_values=None,\n",
    "               train_acc_values=None,\n",
    "               tracking=False\n",
    "               ):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    # model.to(device)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # +++++++\n",
    "        # y_pred shape:  torch.Size([32, 100, 101])\n",
    "        # y shape:  torch.Size([32, 100, 101])\n",
    "        y_pred_logits = model(X)\n",
    "        y_pred = torch.softmax(y_pred_logits, dim=2)\n",
    "        loss = loss_fn(y_pred_logits.permute(0, 2, 1), y.permute(0, 2, 1))\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y.argmax(dim=2), y_pred=y_pred.argmax(dim=2))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    if tracking:\n",
    "        train_loss_values.append(train_loss.clone().detach().cpu().numpy())\n",
    "        train_acc_values.append(train_acc)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module, \n",
    "              accuracy_fn,\n",
    "              device: torch.device = device,\n",
    "              test_loss_values=None,\n",
    "              test_acc_values=None,\n",
    "              tracking=False):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in data_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            test_pred_logits = model(X_test)\n",
    "            test_pred = torch.softmax(test_pred_logits, dim=2)\n",
    "            test_loss += loss_fn(test_pred_logits.permute(0, 2, 1), \n",
    "                                 y_test.permute(0, 2, 1))\n",
    "            test_acc += accuracy_fn(y_true=y_test.argmax(dim=2), \n",
    "                                    y_pred=test_pred.argmax(dim=2))\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader) \n",
    "        if tracking:\n",
    "            test_loss_values.append(test_loss.clone().detach().cpu().numpy())\n",
    "            test_acc_values.append(test_acc)\n",
    "        print(f\"Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[2, 1, 0],\n",
       "          [1, 2, 0]]]),\n",
       " tensor([[[1, 1, 1],\n",
       "          [1, 0, 1]]]),\n",
       " 2,\n",
       " torch.Size([1, 3]),\n",
       " torch.Size([2, 3]),\n",
       " torch.Size([1, 2, 3]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe1 = torch.randint(0, 3, (1, 2, 3))\n",
    "qwe2 = torch.randint(0, 3, (1, 2, 3))\n",
    "correct = torch.eq(qwe1, qwe2).sum().item()\n",
    "qwe1, qwe2, correct, qwe1.argmax(dim=1).shape, qwe1.squeeze(dim=0).shape, qwe1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c66f65a3b7469dae62e7eadea0f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Train loss: 1.26581 | Train acc: 60.66%\n",
      "Test loss: 0.85494, Test acc: 66.02%\n",
      "\n",
      "Epoch: 1\n",
      "------\n",
      "Train loss: 0.86623 | Train acc: 66.13%\n",
      "Test loss: 0.83974, Test acc: 66.61%\n",
      "\n",
      "Epoch: 2\n",
      "------\n",
      "Train loss: 0.84672 | Train acc: 66.59%\n",
      "Test loss: 0.83206, Test acc: 66.95%\n",
      "\n",
      "Epoch: 3\n",
      "------\n",
      "Train loss: 0.83767 | Train acc: 66.76%\n",
      "Test loss: 0.80666, Test acc: 67.41%\n",
      "\n",
      "Epoch: 4\n",
      "------\n",
      "Train loss: 0.83295 | Train acc: 66.78%\n",
      "Test loss: 0.79806, Test acc: 67.40%\n",
      "\n",
      "Epoch: 5\n",
      "------\n",
      "Train loss: 0.82186 | Train acc: 66.96%\n",
      "Test loss: 0.81623, Test acc: 66.76%\n",
      "\n",
      "Epoch: 6\n",
      "------\n",
      "Train loss: 0.82029 | Train acc: 66.94%\n",
      "Test loss: 0.79043, Test acc: 67.56%\n",
      "\n",
      "Epoch: 7\n",
      "------\n",
      "Train loss: 0.81932 | Train acc: 66.93%\n",
      "Test loss: 0.78911, Test acc: 67.41%\n",
      "\n",
      "Epoch: 8\n",
      "------\n",
      "Train loss: 0.81858 | Train acc: 66.98%\n",
      "Test loss: 0.79340, Test acc: 67.23%\n",
      "\n",
      "Epoch: 9\n",
      "------\n",
      "Train loss: 0.81185 | Train acc: 67.16%\n",
      "Test loss: 0.78705, Test acc: 67.60%\n",
      "\n",
      "Epoch: 10\n",
      "------\n",
      "Train loss: 0.80985 | Train acc: 67.21%\n",
      "Test loss: 0.78557, Test acc: 67.71%\n",
      "\n",
      "Epoch: 11\n",
      "------\n",
      "Train loss: 0.80591 | Train acc: 67.31%\n",
      "Test loss: 0.78430, Test acc: 67.68%\n",
      "\n",
      "Epoch: 12\n",
      "------\n",
      "Train loss: 0.80072 | Train acc: 67.45%\n",
      "Test loss: 0.77828, Test acc: 67.84%\n",
      "\n",
      "Epoch: 13\n",
      "------\n",
      "Train loss: 0.80018 | Train acc: 67.51%\n",
      "Test loss: 0.77716, Test acc: 68.06%\n",
      "\n",
      "Epoch: 14\n",
      "------\n",
      "Train loss: 0.79793 | Train acc: 67.53%\n",
      "Test loss: 0.77262, Test acc: 68.04%\n",
      "\n",
      "Epoch: 15\n",
      "------\n",
      "Train loss: 0.79765 | Train acc: 67.56%\n",
      "Test loss: 0.77564, Test acc: 67.70%\n",
      "\n",
      "Epoch: 16\n",
      "------\n",
      "Train loss: 0.79617 | Train acc: 67.56%\n",
      "Test loss: 0.77296, Test acc: 67.79%\n",
      "\n",
      "Epoch: 17\n",
      "------\n",
      "Train loss: 0.79396 | Train acc: 67.63%\n",
      "Test loss: 0.77600, Test acc: 68.15%\n",
      "\n",
      "Epoch: 18\n",
      "------\n",
      "Train loss: 0.79266 | Train acc: 67.75%\n",
      "Test loss: 0.77325, Test acc: 68.00%\n",
      "\n",
      "Epoch: 19\n",
      "------\n",
      "Train loss: 0.79108 | Train acc: 67.76%\n",
      "Test loss: 0.76871, Test acc: 68.21%\n",
      "\n",
      "Epoch: 20\n",
      "------\n",
      "Train loss: 0.79040 | Train acc: 67.74%\n",
      "Test loss: 0.77058, Test acc: 68.31%\n",
      "\n",
      "Epoch: 21\n",
      "------\n",
      "Train loss: 0.79561 | Train acc: 67.64%\n",
      "Test loss: 0.76977, Test acc: 68.21%\n",
      "\n",
      "Epoch: 22\n",
      "------\n",
      "Train loss: 0.79236 | Train acc: 67.70%\n",
      "Test loss: 0.76935, Test acc: 68.21%\n",
      "\n",
      "Epoch: 23\n",
      "------\n",
      "Train loss: 0.78831 | Train acc: 67.77%\n",
      "Test loss: 0.76702, Test acc: 67.95%\n",
      "\n",
      "Epoch: 24\n",
      "------\n",
      "Train loss: 0.78921 | Train acc: 67.79%\n",
      "Test loss: 0.76728, Test acc: 68.26%\n",
      "\n",
      "Epoch: 25\n",
      "------\n",
      "Train loss: 0.78621 | Train acc: 67.80%\n",
      "Test loss: 0.76553, Test acc: 68.25%\n",
      "\n",
      "Epoch: 26\n",
      "------\n",
      "Train loss: 0.79149 | Train acc: 67.68%\n",
      "Test loss: 0.77490, Test acc: 68.09%\n",
      "\n",
      "Epoch: 27\n",
      "------\n",
      "Train loss: 0.78908 | Train acc: 67.71%\n",
      "Test loss: 0.76910, Test acc: 68.14%\n",
      "\n",
      "Epoch: 28\n",
      "------\n",
      "Train loss: 0.78724 | Train acc: 67.78%\n",
      "Test loss: 0.76466, Test acc: 68.28%\n",
      "\n",
      "Epoch: 29\n",
      "------\n",
      "Train loss: 0.78420 | Train acc: 67.84%\n",
      "Test loss: 0.76477, Test acc: 68.27%\n",
      "\n",
      "Epoch: 30\n",
      "------\n",
      "Train loss: 0.78407 | Train acc: 67.84%\n",
      "Test loss: 0.76576, Test acc: 68.13%\n",
      "\n",
      "Epoch: 31\n",
      "------\n",
      "Train loss: 0.78655 | Train acc: 67.76%\n",
      "Test loss: 0.76757, Test acc: 68.06%\n",
      "\n",
      "Epoch: 32\n",
      "------\n",
      "Train loss: 0.78535 | Train acc: 67.77%\n",
      "Test loss: 0.76486, Test acc: 68.28%\n",
      "\n",
      "Epoch: 33\n",
      "------\n",
      "Train loss: 0.78714 | Train acc: 67.80%\n",
      "Test loss: 0.77085, Test acc: 68.22%\n",
      "\n",
      "Epoch: 34\n",
      "------\n",
      "Train loss: 0.78614 | Train acc: 67.81%\n",
      "Test loss: 0.76532, Test acc: 67.97%\n",
      "\n",
      "Epoch: 35\n",
      "------\n",
      "Train loss: 0.78545 | Train acc: 67.78%\n",
      "Test loss: 0.76304, Test acc: 68.28%\n",
      "\n",
      "Epoch: 36\n",
      "------\n",
      "Train loss: 0.78389 | Train acc: 67.84%\n",
      "Test loss: 0.76333, Test acc: 68.35%\n",
      "\n",
      "Epoch: 37\n",
      "------\n",
      "Train loss: 0.78298 | Train acc: 67.84%\n",
      "Test loss: 0.76615, Test acc: 68.40%\n",
      "\n",
      "Epoch: 38\n",
      "------\n",
      "Train loss: 0.78243 | Train acc: 67.87%\n",
      "Test loss: 0.76494, Test acc: 68.21%\n",
      "\n",
      "Epoch: 39\n",
      "------\n",
      "Train loss: 0.78199 | Train acc: 67.88%\n",
      "Test loss: 0.76050, Test acc: 68.41%\n",
      "\n",
      "Epoch: 40\n",
      "------\n",
      "Train loss: 0.78385 | Train acc: 67.81%\n",
      "Test loss: 0.77562, Test acc: 68.26%\n",
      "\n",
      "Epoch: 41\n",
      "------\n",
      "Train loss: 0.78355 | Train acc: 67.85%\n",
      "Test loss: 0.76469, Test acc: 67.86%\n",
      "\n",
      "Epoch: 42\n",
      "------\n",
      "Train loss: 0.78428 | Train acc: 67.83%\n",
      "Test loss: 0.76867, Test acc: 68.11%\n",
      "\n",
      "Epoch: 43\n",
      "------\n",
      "Train loss: 0.78323 | Train acc: 67.88%\n",
      "Test loss: 0.76657, Test acc: 67.97%\n",
      "\n",
      "Epoch: 44\n",
      "------\n",
      "Train loss: 0.78196 | Train acc: 67.86%\n",
      "Test loss: 0.76258, Test acc: 68.28%\n",
      "\n",
      "Epoch: 45\n",
      "------\n",
      "Train loss: 0.78186 | Train acc: 67.86%\n",
      "Test loss: 0.76391, Test acc: 68.46%\n",
      "\n",
      "Epoch: 46\n",
      "------\n",
      "Train loss: 0.78112 | Train acc: 67.88%\n",
      "Test loss: 0.76139, Test acc: 68.29%\n",
      "\n",
      "Epoch: 47\n",
      "------\n",
      "Train loss: 0.78013 | Train acc: 67.90%\n",
      "Test loss: 0.76636, Test acc: 68.21%\n",
      "\n",
      "Epoch: 48\n",
      "------\n",
      "Train loss: 0.78173 | Train acc: 67.92%\n",
      "Test loss: 0.76190, Test acc: 68.39%\n",
      "\n",
      "Epoch: 49\n",
      "------\n",
      "Train loss: 0.78378 | Train acc: 67.79%\n",
      "Test loss: 0.76355, Test acc: 68.46%\n",
      "\n",
      "Epoch: 50\n",
      "------\n",
      "Train loss: 0.78379 | Train acc: 67.79%\n",
      "Test loss: 0.76492, Test acc: 68.32%\n",
      "\n",
      "Epoch: 51\n",
      "------\n",
      "Train loss: 0.78087 | Train acc: 67.85%\n",
      "Test loss: 0.76254, Test acc: 68.19%\n",
      "\n",
      "Epoch: 52\n",
      "------\n",
      "Train loss: 0.77988 | Train acc: 67.93%\n",
      "Test loss: 0.76176, Test acc: 68.24%\n",
      "\n",
      "Epoch: 53\n",
      "------\n",
      "Train loss: 0.77918 | Train acc: 67.89%\n",
      "Test loss: 0.76669, Test acc: 68.24%\n",
      "\n",
      "Epoch: 54\n",
      "------\n",
      "Train loss: 0.78035 | Train acc: 67.95%\n",
      "Test loss: 0.76936, Test acc: 68.05%\n",
      "\n",
      "Epoch: 55\n",
      "------\n",
      "Train loss: 0.78425 | Train acc: 67.82%\n",
      "Test loss: 0.76073, Test acc: 68.51%\n",
      "\n",
      "Epoch: 56\n",
      "------\n",
      "Train loss: 0.77949 | Train acc: 67.97%\n",
      "Test loss: 0.76248, Test acc: 68.36%\n",
      "\n",
      "Epoch: 57\n",
      "------\n",
      "Train loss: 0.78395 | Train acc: 67.80%\n",
      "Test loss: 0.76506, Test acc: 68.26%\n",
      "\n",
      "Epoch: 58\n",
      "------\n",
      "Train loss: 0.78528 | Train acc: 67.81%\n",
      "Test loss: 0.76709, Test acc: 68.09%\n",
      "\n",
      "Epoch: 59\n",
      "------\n",
      "Train loss: 0.78353 | Train acc: 67.88%\n",
      "Test loss: 0.76087, Test acc: 68.36%\n",
      "\n",
      "Epoch: 60\n",
      "------\n",
      "Train loss: 0.77988 | Train acc: 67.94%\n",
      "Test loss: 0.76044, Test acc: 68.23%\n",
      "\n",
      "Epoch: 61\n",
      "------\n",
      "Train loss: 0.77923 | Train acc: 67.98%\n",
      "Test loss: 0.76261, Test acc: 68.26%\n",
      "\n",
      "Epoch: 62\n",
      "------\n",
      "Train loss: 0.78110 | Train acc: 67.93%\n",
      "Test loss: 0.76279, Test acc: 68.42%\n",
      "\n",
      "Epoch: 63\n",
      "------\n",
      "Train loss: 0.78341 | Train acc: 67.86%\n",
      "Test loss: 0.76047, Test acc: 68.41%\n",
      "\n",
      "Epoch: 64\n",
      "------\n",
      "Train loss: 0.77721 | Train acc: 68.01%\n",
      "Test loss: 0.76153, Test acc: 68.46%\n",
      "\n",
      "Epoch: 65\n",
      "------\n",
      "Train loss: 0.77759 | Train acc: 67.97%\n",
      "Test loss: 0.76261, Test acc: 68.44%\n",
      "\n",
      "Epoch: 66\n",
      "------\n",
      "Train loss: 0.78111 | Train acc: 67.86%\n",
      "Test loss: 0.76446, Test acc: 68.36%\n",
      "\n",
      "Epoch: 67\n",
      "------\n",
      "Train loss: 0.77945 | Train acc: 67.93%\n",
      "Test loss: 0.76089, Test acc: 68.37%\n",
      "\n",
      "Epoch: 68\n",
      "------\n",
      "Train loss: 0.78424 | Train acc: 67.89%\n",
      "Test loss: 0.76402, Test acc: 68.26%\n",
      "\n",
      "Epoch: 69\n",
      "------\n",
      "Train loss: 0.78216 | Train acc: 67.90%\n",
      "Test loss: 0.76618, Test acc: 68.38%\n",
      "\n",
      "Epoch: 70\n",
      "------\n",
      "Train loss: 0.78205 | Train acc: 67.85%\n",
      "Test loss: 0.76469, Test acc: 68.08%\n",
      "\n",
      "Epoch: 71\n",
      "------\n",
      "Train loss: 0.77897 | Train acc: 67.88%\n",
      "Test loss: 0.76218, Test acc: 68.41%\n",
      "\n",
      "Epoch: 72\n",
      "------\n",
      "Train loss: 0.77994 | Train acc: 67.92%\n",
      "Test loss: 0.76128, Test acc: 68.38%\n",
      "\n",
      "Epoch: 73\n",
      "------\n",
      "Train loss: 0.77835 | Train acc: 67.99%\n",
      "Test loss: 0.76047, Test acc: 68.44%\n",
      "\n",
      "Epoch: 74\n",
      "------\n",
      "Train loss: 0.78068 | Train acc: 67.90%\n",
      "Test loss: 0.76639, Test acc: 68.51%\n",
      "\n",
      "Epoch: 75\n",
      "------\n",
      "Train loss: 0.77954 | Train acc: 67.94%\n",
      "Test loss: 0.76488, Test acc: 68.18%\n",
      "\n",
      "Epoch: 76\n",
      "------\n",
      "Train loss: 0.77936 | Train acc: 67.92%\n",
      "Test loss: 0.76193, Test acc: 68.41%\n",
      "\n",
      "Epoch: 77\n",
      "------\n",
      "Train loss: 0.77669 | Train acc: 67.99%\n",
      "Test loss: 0.75841, Test acc: 68.39%\n",
      "\n",
      "Epoch: 78\n",
      "------\n",
      "Train loss: 0.77720 | Train acc: 67.93%\n",
      "Test loss: 0.76246, Test acc: 68.51%\n",
      "\n",
      "Epoch: 79\n",
      "------\n",
      "Train loss: 0.77969 | Train acc: 67.93%\n",
      "Test loss: 0.76450, Test acc: 68.22%\n",
      "\n",
      "Epoch: 80\n",
      "------\n",
      "Train loss: 0.78016 | Train acc: 67.94%\n",
      "Test loss: 0.76128, Test acc: 68.48%\n",
      "\n",
      "Epoch: 81\n",
      "------\n",
      "Train loss: 0.78460 | Train acc: 67.81%\n",
      "Test loss: 0.76623, Test acc: 68.32%\n",
      "\n",
      "Epoch: 82\n",
      "------\n",
      "Train loss: 0.78031 | Train acc: 67.83%\n",
      "Test loss: 0.76353, Test acc: 68.34%\n",
      "\n",
      "Epoch: 83\n",
      "------\n",
      "Train loss: 0.77958 | Train acc: 67.90%\n",
      "Test loss: 0.76338, Test acc: 68.34%\n",
      "\n",
      "Epoch: 84\n",
      "------\n",
      "Train loss: 0.77876 | Train acc: 67.93%\n",
      "Test loss: 0.76040, Test acc: 68.60%\n",
      "\n",
      "Epoch: 85\n",
      "------\n",
      "Train loss: 0.77796 | Train acc: 67.99%\n",
      "Test loss: 0.76318, Test acc: 68.47%\n",
      "\n",
      "Epoch: 86\n",
      "------\n",
      "Train loss: 0.78077 | Train acc: 67.87%\n",
      "Test loss: 0.76726, Test acc: 68.19%\n",
      "\n",
      "Epoch: 87\n",
      "------\n",
      "Train loss: 0.78151 | Train acc: 67.90%\n",
      "Test loss: 0.76227, Test acc: 68.48%\n",
      "\n",
      "Epoch: 88\n",
      "------\n",
      "Train loss: 0.78269 | Train acc: 67.90%\n",
      "Test loss: 0.77198, Test acc: 68.30%\n",
      "\n",
      "Epoch: 89\n",
      "------\n",
      "Train loss: 0.78255 | Train acc: 67.85%\n",
      "Test loss: 0.76896, Test acc: 68.13%\n",
      "\n",
      "Epoch: 90\n",
      "------\n",
      "Train loss: 0.77944 | Train acc: 67.90%\n",
      "Test loss: 0.76245, Test acc: 68.29%\n",
      "\n",
      "Epoch: 91\n",
      "------\n",
      "Train loss: 0.77977 | Train acc: 67.91%\n",
      "Test loss: 0.76725, Test acc: 68.30%\n",
      "\n",
      "Epoch: 92\n",
      "------\n",
      "Train loss: 0.78209 | Train acc: 67.88%\n",
      "Test loss: 0.77155, Test acc: 68.11%\n",
      "\n",
      "Epoch: 93\n",
      "------\n",
      "Train loss: 0.78664 | Train acc: 67.68%\n",
      "Test loss: 0.76650, Test acc: 68.23%\n",
      "\n",
      "Epoch: 94\n",
      "------\n",
      "Train loss: 0.78546 | Train acc: 67.78%\n",
      "Test loss: 0.76396, Test acc: 68.44%\n",
      "\n",
      "Epoch: 95\n",
      "------\n",
      "Train loss: 0.78480 | Train acc: 67.81%\n",
      "Test loss: 0.76853, Test acc: 68.25%\n",
      "\n",
      "Epoch: 96\n",
      "------\n",
      "Train loss: 0.78262 | Train acc: 67.88%\n",
      "Test loss: 0.76188, Test acc: 68.46%\n",
      "\n",
      "Epoch: 97\n",
      "------\n",
      "Train loss: 0.77964 | Train acc: 67.90%\n",
      "Test loss: 0.76174, Test acc: 68.52%\n",
      "\n",
      "Epoch: 98\n",
      "------\n",
      "Train loss: 0.77777 | Train acc: 67.87%\n",
      "Test loss: 0.75928, Test acc: 68.43%\n",
      "\n",
      "Epoch: 99\n",
      "------\n",
      "Train loss: 0.77789 | Train acc: 67.92%\n",
      "Test loss: 0.76298, Test acc: 68.37%\n",
      "\n",
      "Train time on cuda: 424.622 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "test_loss_values = []\n",
    "test_acc_values = []\n",
    "epoch_count = []\n",
    "\n",
    "epochs = 100\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n------\")\n",
    "    tracking = False\n",
    "    if epoch % 10 == 0:\n",
    "        tracking = True\n",
    "        epoch_count.append(epoch)\n",
    "    train_step(model=model,\n",
    "               data_loader=train_dataloader,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               accuracy_fn=accuracy_fn,\n",
    "               device=device,\n",
    "               train_loss_values=train_loss_values,\n",
    "               train_acc_values=train_acc_values,\n",
    "               tracking=tracking)\n",
    "    test_step(data_loader=test_dataloader,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_fn,\n",
    "            device=device,\n",
    "            test_loss_values=test_loss_values,\n",
    "            test_acc_values=test_acc_values,\n",
    "            tracking=tracking)\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
    "                                           end=train_time_end_model_2,\n",
    "                                           device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: torch_version_CROSSENTROPYLOSS.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_SAVE_PATH = \"torch_version_CROSSENTROPYLOSS.pth\"\n",
    "\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss & accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/baidu.csv information:\n",
      "torch.Size([1, 100, 100])\n",
      "torch.Size([1, 100, 101])\n",
      "test/gkarate.csv information:\n",
      "torch.Size([1, 100, 100])\n",
      "torch.Size([1, 100, 101])\n",
      "\n",
      "------PREDICTING  test/baidu.csv -------\n",
      "(100, 100)\n",
      "[0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "------PREDICTING  test/gkarate.csv -------\n",
      "(100, 100)\n",
      "[0 0 1 2 3 3 3 3 3 3 0 3 0 3 3 0 0 0 3 0 3 0 3 0 0 0 0 0 0 0 0 0 3 3 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_csvs =[\"test/baidu.csv\", \"test/gkarate.csv\"]\n",
    "\n",
    "x_pred_list = []\n",
    "y_pred_list = []\n",
    "seq_size = 100\n",
    "\n",
    "for i,csv in enumerate(test_csvs):\n",
    "    print(csv, \"information:\")\n",
    "    \n",
    "    seq = pd.read_csv(csv, header=None, low_memory=False)\n",
    "    columns = seq.columns.tolist()\n",
    "    #get the adj edges\n",
    "    adj_edge = np.array(seq[columns[1:2*seq_size+1]])\n",
    "    #get the colors\n",
    "    data_color = np.array(seq[columns[2*seq_size+1:3*seq_size+1]])\n",
    "    X = np.zeros((adj_edge.shape[0], data_color.shape[1], seq_size))\n",
    "    X, Y = adBits(adj_edge, X, data_color)\n",
    "    Y = updateLabelBits(X, Y)\n",
    "    Y = np.eye(101, dtype='float32')[Y]\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "\n",
    "    x_pred_list.append(X)\n",
    "    y_pred_list.append(Y)\n",
    "\n",
    "    print(x_pred_list[i].shape)\n",
    "    print(y_pred_list[i].shape)  \n",
    "\n",
    "\n",
    "for i,csv in enumerate(test_csvs):\n",
    "    print('\\n------PREDICTING ',csv,'-------')\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        predicted = model(x_pred_list[i]) # torch.Size([1, 100, 101])\n",
    "        predicted = torch.softmax(predicted, dim=2)\n",
    "        predicted = predicted.squeeze(dim=0).clone().detach().cpu().numpy() # (100, 101)\n",
    "        print(predicted[:,1:].shape)\n",
    "        print(np.argmax(predicted[:,1:], axis=1))\n",
    "\n",
    "        # x_pred = x_pred_list[i].clone().detach().cpu() # torch.Size([1, 100, 100])\n",
    "        # print('\\nInvalid edges percentage before color correction ->')\n",
    "        # post_process(np.asarray(x_pred.tolist()), predicted[:,1:])\n",
    "        \n",
    "        # print('\\nColors list and Chromatic number predicted by the model ->')\n",
    "        # colors_list_list_before_correction = post_process_chromatic(np.asarray(x_pred), predicted)\n",
    "        # print('\\nApply color correction ->')\n",
    "        # predicted = post_process_correction(np.asarray(x_pred), predicted, colors_list_list_before_correction)\n",
    "        # print('\\nColors list and Chromatic number following color correction ->')\n",
    "        # colors_list_list_after_correction = post_process_chromatic(np.asarray(x_pred), predicted)\n",
    "        # print('\\nInvalid edges percentage after color correction ->')\n",
    "        # post_process(np.asarray(x_pred), predicted)\n",
    "        # create_csv_rows(csv.rsplit('/',1)[1], colors_list_list_before_correction, colors_list_list_after_correction)\n",
    "\n",
    "# import csv\n",
    "# with open(\"Result/pytorch_graph_coloring_result.csv\", 'w') as csvfile:\n",
    "#     # creating a csv writer object\n",
    "#     csvwriter = csv.writer(csvfile)      \n",
    "#     # writing the data rows\n",
    "#     csvwriter.writerows(csv_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs583",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
